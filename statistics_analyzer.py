"""
ç»Ÿè®¡åˆ†ææ¨¡å— - å¤„ç†å•è¯åŠ å…¥æ—¶é—´ç»Ÿè®¡ã€çƒ­åŠ›å›¾ç­‰
"""

from datetime import datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Tuple, Optional


class StatisticsAnalyzer:
    """ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»Ÿè®¡åˆ†æå™¨"""
        pass
    
    @staticmethod
    def get_daily_statistics(words_data: List[Tuple[str, str, Optional[str]]]) -> Dict[str, int]:
        """
        æŒ‰æ—¥æœŸç»Ÿè®¡å•è¯æ•°é‡
        
        Args:
            words_data: [(å•è¯, é‡Šä¹‰, æ—¶é—´æˆ³), ...] åˆ—è¡¨
        
        Returns:
            {æ—¥æœŸ(YYYY-MM-DD): å•è¯æ•°} å­—å…¸
        """
        daily_count = defaultdict(int)
        
        for word, definition, timestamp in words_data:
            if timestamp:
                try:
                    # ä»ISOæ ¼å¼æ—¶é—´æˆ³æå–æ—¥æœŸ
                    dt = datetime.fromisoformat(timestamp)
                    date_str = dt.strftime("%Y-%m-%d")
                    daily_count[date_str] += 1
                except Exception:
                    pass
        
        return dict(sorted(daily_count.items()))
    
    @staticmethod
    def generate_heatmap_text(daily_stats: Dict[str, int]) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„çƒ­åŠ›å›¾
        
        Args:
            daily_stats: {æ—¥æœŸ: æ•°é‡} å­—å…¸
        
        Returns:
            çƒ­åŠ›å›¾æ–‡æœ¬
        """
        if not daily_stats:
            return "æš‚æ— ç»Ÿè®¡æ•°æ®"
        
        # æŒ‰æ—¥æœŸæ’åº
        sorted_dates = sorted(daily_stats.items())
        
        # è®¡ç®—çƒ­åº¦ç­‰çº§
        max_count = max(daily_stats.values()) if daily_stats else 1
        
        result = "ğŸ“Š æ¯æ—¥åŠ å…¥ç»Ÿè®¡çƒ­åŠ›å›¾\n"
        result += "="*60 + "\n"
        
        current_date = None
        week_data = []
        
        for date_str, count in sorted_dates:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d")
            
            # æŒ‰å‘¨åˆ†ç»„
            if current_date is None:
                current_date = date_obj
            
            # è®¡ç®—çƒ­åº¦ç­‰çº§ï¼ˆ0-4ï¼‰
            heat_level = int((count / max_count) * 4) if max_count > 0 else 0
            
            # çƒ­åº¦ç¬¦å·
            heat_symbols = ['â¬œ', 'ğŸŸ©', 'ğŸŸ©', 'ğŸŸ¦', 'ğŸŸ§', 'ğŸŸ¥']
            heat_symbol = heat_symbols[heat_level] if heat_level < len(heat_symbols) else 'ğŸŸ¥'
            
            week_data.append((date_str, heat_symbol, count))
            
            # æ¯7è¡Œè¾“å‡ºä¸€ç»„ï¼ˆå‘¨ï¼‰
            if len(week_data) % 7 == 0 or date_str == sorted_dates[-1][0]:
                for d, s, c in week_data:
                    result += f"{s} {d}: {c} ä¸ªå•è¯\n"
                if len(week_data) % 7 == 0:
                    result += "-"*60 + "\n"
                week_data = []
        
        # ç»Ÿè®¡æ€»æ•°
        total_count = sum(daily_stats.values())
        result += "="*60 + "\n"
        result += f"æ€»è®¡: {total_count} ä¸ªå•è¯\n"
        result += f"ç»Ÿè®¡å¤©æ•°: {len(daily_stats)} å¤©\n"
        result += f"å¹³å‡æ¯å¤©: {total_count/len(daily_stats):.1f} ä¸ª\n"
        
        return result
    
    @staticmethod
    def get_word_with_timestamp(words_data: List[Tuple[str, str, Optional[str]]]) -> str:
        """
        è·å–å¸¦æ—¶é—´æˆ³çš„å•è¯åˆ—è¡¨
        
        Args:
            words_data: [(å•è¯, é‡Šä¹‰, æ—¶é—´æˆ³), ...] åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å•è¯æ—¶é—´åˆ—è¡¨
        """
        if not words_data:
            return "æš‚æ— å•è¯"
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_words = sorted(
            [(w, d, t) for w, d, t in words_data if t],
            key=lambda x: x[2],
            reverse=True
        )
        
        result = "ğŸ“ å•è¯åŠ å…¥æ—¶é—´è®°å½•\n"
        result += "="*60 + "\n"
        
        for word, definition, timestamp in sorted_words[:100]:  # æ˜¾ç¤ºæœ€æ–°çš„100ä¸ª
            try:
                dt = datetime.fromisoformat(timestamp)
                time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                result += f"â° {time_str}: {word}\n"
                if definition:
                    result += f"   ğŸ“Œ {definition}\n"
            except Exception:
                pass
        
        if len(sorted_words) > 100:
            result += f"\n... è¿˜æœ‰ {len(sorted_words) - 100} ä¸ªå•è¯"
        
        return result
    
    @staticmethod
    def get_time_range_statistics(words_data: List[Tuple[str, str, Optional[str]]],
                                   start_date: str = None,
                                   end_date: str = None) -> Tuple[bool, str]:
        """
        è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ç»Ÿè®¡
        
        Args:
            words_data: [(å•è¯, é‡Šä¹‰, æ—¶é—´æˆ³), ...] åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            (æˆåŠŸä¸å¦, ç»“æœå­—ç¬¦ä¸²)
        """
        if not words_data:
            return True, "æš‚æ— å•è¯æ•°æ®"
        
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d") if start_date else None
            end = datetime.strptime(end_date, "%Y-%m-%d") if end_date else None
            
            filtered_words = []
            for word, definition, timestamp in words_data:
                if timestamp:
                    try:
                        dt = datetime.fromisoformat(timestamp)
                        if (start is None or dt >= start) and (end is None or dt <= end):
                            filtered_words.append((word, definition, timestamp))
                    except Exception:
                        pass
            
            if not filtered_words:
                return True, f"æ—¶é—´èŒƒå›´ {start_date} åˆ° {end_date} å†…æ— å•è¯"
            
            result = f"ğŸ“Š æ—¶é—´èŒƒå›´ç»Ÿè®¡: {start_date} è‡³ {end_date}\n"
            result += f"å…± {len(filtered_words)} ä¸ªå•è¯:\n\n"
            
            for word, definition, _ in filtered_words[:50]:
                result += f"â€¢ {word}: {definition}\n"
            
            if len(filtered_words) > 50:
                result += f"\n... è¿˜æœ‰ {len(filtered_words) - 50} ä¸ªå•è¯"
            
            return True, result
        
        except ValueError as e:
            return False, f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼"
    
    @staticmethod
    def get_trending_words(words_data: List[Tuple[str, str, Optional[str]]], 
                          days: int = 7) -> str:
        """
        è·å–æœ€è¿‘Nå¤©æ·»åŠ çš„å•è¯
        
        Args:
            words_data: [(å•è¯, é‡Šä¹‰, æ—¶é—´æˆ³), ...] åˆ—è¡¨
            days: å¤©æ•°
        
        Returns:
            ç»“æœå­—ç¬¦ä¸²
        """
        if not words_data:
            return f"æš‚æ— è¿‡å» {days} å¤©çš„å•è¯æ•°æ®"
        
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_words = []
        
        for word, definition, timestamp in words_data:
            if timestamp:
                try:
                    dt = datetime.fromisoformat(timestamp)
                    if dt >= cutoff_date:
                        recent_words.append((word, definition, timestamp))
                except Exception:
                    pass
        
        if not recent_words:
            return f"è¿‡å» {days} å¤©å†…æ— æ–°å¢å•è¯"
        
        # æŒ‰æ—¶é—´æ’åº
        recent_words = sorted(recent_words, key=lambda x: x[2], reverse=True)
        
        result = f"ğŸ“ˆ è¿‡å» {days} å¤©çš„æ–°å¢å•è¯ (å…± {len(recent_words)} ä¸ª):\n"
        result += "="*60 + "\n"
        
        for word, definition, timestamp in recent_words[:50]:
            try:
                dt = datetime.fromisoformat(timestamp)
                time_str = dt.strftime("%m-%d %H:%M")
                result += f"â€¢ {word:20} ({time_str}): {definition}\n"
            except Exception:
                pass
        
        if len(recent_words) > 50:
            result += f"\n... è¿˜æœ‰ {len(recent_words) - 50} ä¸ªå•è¯"
        
        return result
